# YAML file for Paperspace Gradient NLP Text Generation Tutorial example
# CPU-safe configuration with distilgpt2 model
# Updated to use C5 instances and CPU-only PyTorch for cost efficiency
#
# The Workflow is triggered when this is present in the .gradient/workflows/ directory in a GitHub
# repository linked to the user's Gradient project
# It clones this repo and then in turn calls the file nlp_text_generation.py
# This file outputs the generated text to output.txt in a Gradient-managed Dataset
#
# See the Gradient documentation page for more details:
# https://docs.paperspace.com/gradient/get-started/tutorials-list/example-workflow-nlp-text-generator
#
# Last updated: August 2025
on:
  github:
    branches:
      only: main
jobs:
  cloneRepo:
    resources:
      instance-type: C5
    outputs:
      repo:
        type: volume
    uses: git-checkout@v1
    with:
      url: context.event.github.url
      ref: context.event.github.ref
  generateText:
    resources:
      instance-type: C5
    needs:
      - cloneRepo
    inputs:
      repo: cloneRepo.outputs.repo
    outputs:
      generatedText:
        type: dataset
        with:
          ref: demo-dataset
    uses: script@v1
    with:
      script: |-
        #!/bin/bash
        set -euxo pipefail
        
        echo "Starting text generation workflow with verbose logging"
        echo "Timestamp: $(date)"
        echo "Working directory: $(pwd)"
        echo "Available memory: $(free -h)"
        echo "CPU info: $(nproc) cores"
        
        # Ensure Python tools are available
        apt-get update && apt-get install -y python3-pip python3-venv
        
        echo "Copying repository to working directory"
        cp -R /inputs/repo /nlp
        cd /nlp
        echo "Repository contents:"
        ls -la
        
        # Install CPU-only PyTorch and transformers for cost-efficient processing
        echo "Installing CPU-only PyTorch and dependencies"
        python3 -m pip install --upgrade pip
        python3 -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
        python3 -m pip install transformers>=4.30.0 datasets accelerate
        
        echo "Verifying PyTorch installation (CPU-only):"
        python3 -c "import torch; print(f'PyTorch version: {torch.__version__}'); print(f'CUDA available: {torch.cuda.is_available()}'); print('Using CPU-only configuration for cost efficiency')"
        
        # Create a simple generation script for distilgpt2
        echo "Creating text generation script with distilgpt2 model"
        cat > generate_text.py << 'EOF'
#!/usr/bin/env python3
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

print("Loading distilgpt2 model and tokenizer...")
model_name = "distilgpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Set pad token
tokenizer.pad_token = tokenizer.eos_token

print("Generating text...")
input_text = "The future of artificial intelligence is"
inputs = tokenizer.encode(input_text, return_tensors="pt")

# Generate text
with torch.no_grad():
    outputs = model.generate(
        inputs,
        max_length=200,
        num_return_sequences=1,
        temperature=0.8,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(f"Generated text: {generated_text}")

# Write to output.txt
with open("output.txt", "w") as f:
    f.write(generated_text)
    
print("Text generation completed successfully!")
EOF
        
        echo "Running text generation with distilgpt2"
        python3 generate_text.py
        
        echo "Moving output to dataset"
        ls -la output.txt
        mv output.txt /outputs/generatedText/
        
        echo "Final output verification:"
        ls -aFlR /outputs/generatedText
        echo "Workflow completed successfully at $(date)"
      image: paperspace/pytorch:2.1.0-cuda11.8
# Appendix: Extra details
#
# Updated to use C5 CPU instances for cost-efficient processing
# Uses CPU-only PyTorch installation to avoid GPU costs
# Script includes set -euxo pipefail for robust error handling
# Added verbose logging throughout the process
# Uses distilgpt2 as the default model for faster, lighter processing
# Generates output.txt and places it in demo-dataset as requested
